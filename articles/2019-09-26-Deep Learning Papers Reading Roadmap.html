<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.38">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <title>Deep Learning Papers | Laqudee.Z - F2E</title><meta name="description" content="使用 Vuepress + Github Pages实现的博客">
    <link rel="modulepreload" href="/assets/app.3547d211.js"><link rel="modulepreload" href="/assets/2019-09-26-Deep Learning Papers Reading Roadmap.html.671619fe.js"><link rel="modulepreload" href="/assets/2019-09-26-Deep Learning Papers Reading Roadmap.html.a8deb1f5.js">
    <link rel="stylesheet" href="/assets/style.a593710e.css">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a href="/" class=""><img class="logo" src="/images/logo.jpg" alt="Laqudee.Z - F2E"><span class="site-name can-hide">Laqudee.Z - F2E</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><a href="/" class="" aria-label="Home"><!--[--><!--]--> Home <!--[--><!--]--></a></div><div class="navbar-item"><a href="/articles/" class="router-link-active" aria-label="Articles"><!--[--><!--]--> Articles <!--[--><!--]--></a></div><div class="navbar-item"><a href="/projects/" class="" aria-label="Projects"><!--[--><!--]--> Projects <!--[--><!--]--></a></div><div class="navbar-item"><a href="/joyExperiment/" class="" aria-label="Joy"><!--[--><!--]--> Joy <!--[--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="More"><span class="title">More</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="More"><span class="title">More</span><span class="right arrow"></span></button><!--[--><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="external-link" href="https://github.com/ZhaoYLong" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a class="external-link" href="https://twitter.com/Laqudee1" rel="noopener noreferrer" target="_blank" aria-label="Twitter"><!--[--><!--]--> Twitter <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></li><!--]--></ul><!--]--></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/ZhaoYLong/vuepress-starter" rel="noopener noreferrer" target="_blank" aria-label="GitHub"><!--[--><!--]--> GitHub <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button class="toggle-dark-button" title="toggle dark mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><a href="/" class="" aria-label="Home"><!--[--><!--]--> Home <!--[--><!--]--></a></div><div class="navbar-item"><a href="/articles/" class="router-link-active" aria-label="Articles"><!--[--><!--]--> Articles <!--[--><!--]--></a></div><div class="navbar-item"><a href="/projects/" class="" aria-label="Projects"><!--[--><!--]--> Projects <!--[--><!--]--></a></div><div class="navbar-item"><a href="/joyExperiment/" class="" aria-label="Joy"><!--[--><!--]--> Joy <!--[--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="More"><span class="title">More</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="More"><span class="title">More</span><span class="right arrow"></span></button><!--[--><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="external-link" href="https://github.com/ZhaoYLong" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a class="external-link" href="https://twitter.com/Laqudee1" rel="noopener noreferrer" target="_blank" aria-label="Twitter"><!--[--><!--]--> Twitter <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></li><!--]--></ul><!--]--></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/ZhaoYLong/vuepress-starter" rel="noopener noreferrer" target="_blank" aria-label="GitHub"><!--[--><!--]--> GitHub <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading active collapsible">笔记 <span class="down arrow"></span></p><!--[--><ul style="" class="sidebar-item-children"><!--[--><li><a href="/articles/2018-08-25-D3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="D3.js学习笔记"><!--[--><!--]--> D3.js学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2018-10-13-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E9%9D%A2%E8%AF%95.html" class="sidebar-item" aria-label="操作系统基础知识之面试"><!--[--><!--]--> 操作系统基础知识之面试 <!--[--><!--]--></a><!----></li><li><a href="/articles/2018-11-24-Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="Redis学习笔记"><!--[--><!--]--> Redis学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2018-12-01-Redis%E9%9D%A2%E8%AF%95.html" class="sidebar-item" aria-label="Redis之面试"><!--[--><!--]--> Redis之面试 <!--[--><!--]--></a><!----></li><li><a href="/articles/2018-12-11-MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="MySQL学习笔记"><!--[--><!--]--> MySQL学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2018-12-16-Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html" class="sidebar-item" aria-label="Java基础知识"><!--[--><!--]--> Java基础知识 <!--[--><!--]--></a><!----></li><li><a href="/articles/2018-12-18-Java%E9%9B%86%E5%90%88%E7%B1%BB%E5%AD%A6%E4%B9%A0.html" class="sidebar-item" aria-label="Java集合类学习"><!--[--><!--]--> Java集合类学习 <!--[--><!--]--></a><!----></li><li><a href="/articles/2018-12-25-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%81Python%E9%A2%98%E5%BA%93.html" class="sidebar-item" aria-label="机器学习、大数据、Python试题库"><!--[--><!--]--> 机器学习、大数据、Python试题库 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-01-06-Java%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80%E6%A2%B3%E7%90%86.html" class="sidebar-item" aria-label="Java并发基础梳理"><!--[--><!--]--> Java并发基础梳理 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-01-08-Java%E5%B9%B6%E5%8F%91%E5%AD%A6%E4%B9%A0.html" class="sidebar-item" aria-label="Java并发学习"><!--[--><!--]--> Java并发学习 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-01-10-Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91.html" class="sidebar-item" aria-label="Java多线程与并发"><!--[--><!--]--> Java多线程与并发 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-01-17-Java%E9%9B%86%E5%90%88%E7%B1%BB%E6%80%BB%E7%BB%93.html" class="sidebar-item" aria-label="Java集合类总结"><!--[--><!--]--> Java集合类总结 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-02-03-Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6.html" class="sidebar-item" aria-label="Java内存模型与垃圾回收"><!--[--><!--]--> Java内存模型与垃圾回收 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-03-04-%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AD%89%E6%8A%80%E6%9C%AF%E9%9D%A2%E8%AF%95.html" class="sidebar-item" aria-label="数据库、多线程等技术面试"><!--[--><!--]--> 数据库、多线程等技术面试 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-04-26-Hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0.html" class="sidebar-item" aria-label="Hadoop大数据处理入门"><!--[--><!--]--> Hadoop大数据处理入门 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-05-13-NLP%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6(Part.1).html" class="sidebar-item" aria-label="NLP算法研究，从入门到入坟"><!--[--><!--]--> NLP算法研究，从入门到入坟 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-05-23-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E8%BF%B0.html" class="sidebar-item" aria-label="卷积神经网络简述"><!--[--><!--]--> 卷积神经网络简述 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-05-24-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html" class="sidebar-item" aria-label="神经网络简要介绍"><!--[--><!--]--> 神经网络简要介绍 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-06-12-SSM%E6%A1%86%E6%9E%B6%E5%8F%8ASpringBoot%E6%95%B4%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="SSM框架及Spring Boot整合学习"><!--[--><!--]--> SSM框架及Spring Boot整合学习 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-06-16-Vue%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86.html" class="sidebar-item" aria-label="Vue.js学习笔记之基础部分"><!--[--><!--]--> Vue.js学习笔记之基础部分 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-06-18-Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html" class="sidebar-item" aria-label="设计模式学习笔记"><!--[--><!--]--> 设计模式学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-06-20-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F.html" class="sidebar-item" aria-label="设计模式学习笔记"><!--[--><!--]--> 设计模式学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-06-23-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%81%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE(Java).html" class="sidebar-item" aria-label="数据结构与算法分析回顾"><!--[--><!--]--> 数据结构与算法分析回顾 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-08-19-Front%20End%20And%20Idea.html" class="sidebar-item" aria-label="Front End And Idea"><!--[--><!--]--> Front End And Idea <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-09-16-%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3DVA%E6%BA%90%E7%A0%81.html" class="sidebar-item" aria-label="深度理解DVA源码"><!--[--><!--]--> 深度理解DVA源码 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-09-17-roadhog%E8%A7%A3%E6%9E%90.html" class="sidebar-item" aria-label="roadhog简单解析"><!--[--><!--]--> roadhog简单解析 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-09-19-%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3DVA%E6%BA%90%E7%A0%81.html" class="sidebar-item" aria-label="深度理解DVA源码"><!--[--><!--]--> 深度理解DVA源码 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-09-21-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E8%BF%B0.html" class="sidebar-item" aria-label="卷积神经网络简述"><!--[--><!--]--> 卷积神经网络简述 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-09-21-%E7%A7%8B%E8%82%A5%E8%BF%B9.html" class="sidebar-item" aria-label="秋肥迹"><!--[--><!--]--> 秋肥迹 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-09-25-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E8%BF%B0.html" class="sidebar-item" aria-label="卷积神经网络简述"><!--[--><!--]--> 卷积神经网络简述 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html" class="router-link-active router-link-exact-active router-link-active sidebar-item active" aria-label="Deep Learning Papers"><!--[--><!--]--> Deep Learning Papers <!--[--><!--]--></a><!--[--><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_1-0-book" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.0 Book"><!--[--><!--]--> 1.0 Book <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_1-1-survey" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.1 Survey"><!--[--><!--]--> 1.1 Survey <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_1-2-deep-belief-network-dbn-milestone-of-deep-learning-eve" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)"><!--[--><!--]--> 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve) <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_1-3-imagenet-evolution-deep-learning-broke-out-from-here" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.3 ImageNet Evolution（Deep Learning broke out from here）"><!--[--><!--]--> 1.3 ImageNet Evolution（Deep Learning broke out from here） <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_1-4-speech-recognition-evolution" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.4 Speech Recognition Evolution"><!--[--><!--]--> 1.4 Speech Recognition Evolution <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-1-model" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.1 Model"><!--[--><!--]--> 2.1 Model <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-2-optimization" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.2 Optimization"><!--[--><!--]--> 2.2 Optimization <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-3-unsupervised-learning-deep-generative-model" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.3 Unsupervised Learning / Deep Generative Model"><!--[--><!--]--> 2.3 Unsupervised Learning / Deep Generative Model <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-4-rnn-sequence-to-sequence-model" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.4 RNN / Sequence-to-Sequence Model"><!--[--><!--]--> 2.4 RNN / Sequence-to-Sequence Model <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-5-neural-turing-machine" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.5 Neural Turing Machine"><!--[--><!--]--> 2.5 Neural Turing Machine <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-6-deep-reinforcement-learning" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.6 Deep Reinforcement Learning"><!--[--><!--]--> 2.6 Deep Reinforcement Learning <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-7-deep-transfer-learning-lifelong-learning-especially-for-rl" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.7 Deep Transfer Learning / Lifelong Learning / especially for RL"><!--[--><!--]--> 2.7 Deep Transfer Learning / Lifelong Learning / especially for RL <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_2-8-one-shot-deep-learning" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.8 One Shot Deep Learning"><!--[--><!--]--> 2.8 One Shot Deep Learning <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-1-nlp-natural-language-processing" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.1 NLP(Natural Language Processing)"><!--[--><!--]--> 3.1 NLP(Natural Language Processing) <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-2-object-detection" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.2 Object Detection"><!--[--><!--]--> 3.2 Object Detection <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-3-visual-tracking" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.3 Visual Tracking"><!--[--><!--]--> 3.3 Visual Tracking <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-4-image-caption" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.4 Image Caption"><!--[--><!--]--> 3.4 Image Caption <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-5-machine-translation" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.5 Machine Translation"><!--[--><!--]--> 3.5 Machine Translation <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-6-robotics" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.6 Robotics"><!--[--><!--]--> 3.6 Robotics <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-7-art" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.7 Art"><!--[--><!--]--> 3.7 Art <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/articles/2019-09-26-Deep%20Learning%20Papers%20Reading%20Roadmap.html#_3-8-object-segmentation" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.8 Object Segmentation"><!--[--><!--]--> 3.8 Object Segmentation <!--[--><!--]--></a><!----></li><!--]--></ul><!--]--></li><li><a href="/articles/2019-09-26-Learn%20Math%20Fast.html" class="sidebar-item" aria-label="Learn Math Fast"><!--[--><!--]--> Learn Math Fast <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-10-11-Redux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="React通信利器--Redux"><!--[--><!--]--> React通信利器--Redux <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-10-12-dva%E6%A6%82%E5%BF%B5%E6%A2%B3%E7%90%86.html" class="sidebar-item" aria-label="DVA概念梳理[一遍通]"><!--[--><!--]--> DVA概念梳理[一遍通] <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-10-12-python%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86sphfile%E6%A0%BC%E5%BC%8F%E8%AF%AD%E9%9F%B3%E6%96%87%E4%BB%B6.html" class="sidebar-item" aria-label="Python &amp; Sphfile格式"><!--[--><!--]--> Python &amp; Sphfile格式 <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-10-21-%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%98%AF%E4%B8%AA%E4%BB%80%E4%B9%88%E9%BB%91%E9%AD%94%E6%B3%95.html" class="sidebar-item" aria-label="语音识别是个什么黑魔法？"><!--[--><!--]--> 语音识别是个什么黑魔法？ <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-10-9-ReduxSaga%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="Redux-Saga是什么？"><!--[--><!--]--> Redux-Saga是什么？ <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-11-01-git%E6%8A%80%E8%83%BD%E5%A4%A7%E6%B3%95%E5%AD%A6%E4%B9%A0.html" class="sidebar-item" aria-label="Git技能学习笔记(第一篇)"><!--[--><!--]--> Git技能学习笔记(第一篇) <!--[--><!--]--></a><!----></li><li><a href="/articles/2019-12-29-%E6%9C%89%E8%B6%A3%E7%9A%84%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95.html" class="sidebar-item" aria-label="有趣的开源项目记录"><!--[--><!--]--> 有趣的开源项目记录 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-02-27-flutterNote.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-03-01-flutterNote02.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-05-24-nodeJS%E5%AD%A6%E4%B9%A0%E4%BA%8C.html" class="sidebar-item" aria-label="node学习笔记"><!--[--><!--]--> node学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-05-24-nodeJS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="node学习笔记"><!--[--><!--]--> node学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-05-27-nodejs%E5%AD%A6%E4%B9%A0%E4%B8%89.html" class="sidebar-item" aria-label="node学习笔记"><!--[--><!--]--> node学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-05-29-nodeJS%E5%9B%9B.html" class="sidebar-item" aria-label="node学习笔记"><!--[--><!--]--> node学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-06-02-nodeJS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94.html" class="sidebar-item" aria-label="node学习笔记"><!--[--><!--]--> node学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-06-07-nodeJS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD.html" class="sidebar-item" aria-label="node学习笔记"><!--[--><!--]--> node学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-06-28-flutterNote03.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-07-01-flutterNote04.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-07-02-flutterNote05.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-07-07-flutterNote06.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-07-13-flutterNote07.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-07-14-flutterNote08.html" class="sidebar-item" aria-label="Flutter学习笔记"><!--[--><!--]--> Flutter学习笔记 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-03-CLI%E6%9C%8D%E5%8A%A1.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-03-preset.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-03-%E6%B5%8F%E8%A7%88%E5%99%A8%E5%85%BC%E5%AE%B9.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-04-CSS%E7%9B%B8%E5%85%B3.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-04-webpack%E7%9B%B8%E5%85%B3.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-04-%E6%9E%84%E5%BB%BA%E7%9B%AE%E6%A0%87.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-04-%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E6%A8%A1%E5%BC%8F.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-04-%E9%83%A8%E7%BD%B2.html" class="sidebar-item" aria-label="Vue-Cli Think"><!--[--><!--]--> Vue-Cli Think <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-26-Vue2%E5%92%8CEcharts%E7%9A%84%E5%A5%87%E5%A6%99%E5%8F%8D%E5%BA%94.html" class="sidebar-item" aria-label="Vue2和Echarts的奇妙反应"><!--[--><!--]--> Vue2和Echarts的奇妙反应 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-26-Vue2%E5%92%8CHightCharts.html" class="sidebar-item" aria-label="Vue2和HighCharts的奇妙反应"><!--[--><!--]--> Vue2和HighCharts的奇妙反应 <!--[--><!--]--></a><!----></li><li><a href="/articles/2020-12-29-Vue3%E7%BB%84%E5%90%88%E5%BC%8FAPI%E6%84%8F%E8%A7%81%E7%A8%BF.html" class="sidebar-item" aria-label="Vue3组合式API意见稿"><!--[--><!--]--> Vue3组合式API意见稿 <!--[--><!--]--></a><!----></li><li><a href="/articles/2021-01-18-webpack%E7%AC%94%E8%AE%B0.html" class="sidebar-item" aria-label="webpack课程记录"><!--[--><!--]--> webpack课程记录 <!--[--><!--]--></a><!----></li><li><a href="/articles/2021-01-18-%E7%81%B5%E5%85%89%E4%B9%8D%E7%8E%B0.html" class="sidebar-item" aria-label="Front End And Idea"><!--[--><!--]--> Front End And Idea <!--[--><!--]--></a><!----></li><li><a href="/articles/" class="router-link-active sidebar-item" aria-label="如何使用vuePress"><!--[--><!--]--> 如何使用vuePress <!--[--><!--]--></a><!----></li><!--]--></ul><!--]--></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><h1 id="deep-learning-papers-reading-roadmap" tabindex="-1"><a class="header-anchor" href="#deep-learning-papers-reading-roadmap" aria-hidden="true">#</a> Deep Learning Papers Reading Roadmap</h1><blockquote><p><a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap" target="_blank" rel="noopener noreferrer">fake from floodsung at github,thanks!<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><blockquote><p>If you are a newcomer to the Deep Learning area, the first question you may have is &quot;Which paper should I start reading from?&quot;</p></blockquote><blockquote><p>Here is a reading roadmap of Deep Learning papers!</p></blockquote><p>The roadmap is constructed in accordance with the following four guidelines:</p><ul><li>From outline to detail</li><li>From old to state-of-the-art</li><li>from generic to specific areas</li><li>focus on state-of-the-art</li></ul><p>You will find many papers that are quite new but really worth reading.</p><p>I would continue adding papers to this roadmap.</p><hr><h1 id="_1-deep-learning-history-and-basics" tabindex="-1"><a class="header-anchor" href="#_1-deep-learning-history-and-basics" aria-hidden="true">#</a> 1 Deep Learning History and Basics</h1><h2 id="_1-0-book" tabindex="-1"><a class="header-anchor" href="#_1-0-book" aria-hidden="true">#</a> 1.0 Book</h2><p><strong>[0]</strong> Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. &quot;<strong>Deep learning</strong>.&quot; An MIT Press book. (2015). <a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener noreferrer">[html]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Deep Learning Bible, you can read this book while reading following papers.)</strong> ⭐⭐⭐⭐⭐</p><h2 id="_1-1-survey" tabindex="-1"><a class="header-anchor" href="#_1-1-survey" aria-hidden="true">#</a> 1.1 Survey</h2><p><strong>[1]</strong> LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. &quot;<strong>Deep learning</strong>.&quot; Nature 521.7553 (2015): 436-444. <a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Three Giants&#39; Survey)</strong> ⭐⭐⭐⭐⭐</p><h2 id="_1-2-deep-belief-network-dbn-milestone-of-deep-learning-eve" tabindex="-1"><a class="header-anchor" href="#_1-2-deep-belief-network-dbn-milestone-of-deep-learning-eve" aria-hidden="true">#</a> 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)</h2><p><strong>[2]</strong> Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. &quot;<strong>A fast learning algorithm for deep belief nets</strong>.&quot; Neural computation 18.7 (2006): 1527-1554. <a href="http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><strong>(Deep Learning Eve)</strong> ⭐⭐⭐</p><p><strong>[3]</strong> Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. &quot;<strong>Reducing the dimensionality of data with neural networks</strong>.&quot; Science 313.5786 (2006): 504-507. <a href="http://www.cs.toronto.edu/~hinton/science.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Milestone, Show the promise of deep learning)</strong> ⭐⭐⭐</p><h2 id="_1-3-imagenet-evolution-deep-learning-broke-out-from-here" tabindex="-1"><a class="header-anchor" href="#_1-3-imagenet-evolution-deep-learning-broke-out-from-here" aria-hidden="true">#</a> 1.3 ImageNet Evolution（Deep Learning broke out from here）</h2><p><strong>[4]</strong> Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &quot;<strong>Imagenet classification with deep convolutional neural networks</strong>.&quot; Advances in neural information processing systems. 2012. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(AlexNet, Deep Learning Breakthrough)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[5]</strong> Simonyan, Karen, and Andrew Zisserman. &quot;<strong>Very deep convolutional networks for large-scale image recognition</strong>.&quot; arXiv preprint arXiv:1409.1556 (2014). <a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(VGGNet,Neural Networks become very deep!)</strong> ⭐⭐⭐</p><p><strong>[6]</strong> Szegedy, Christian, et al. &quot;<strong>Going deeper with convolutions</strong>.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(GoogLeNet)</strong> ⭐⭐⭐</p><p><strong>[7]</strong> He, Kaiming, et al. &quot;<strong>Deep residual learning for image recognition</strong>.&quot; arXiv preprint arXiv:1512.03385 (2015). <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(ResNet,Very very deep networks, CVPR best paper)</strong> ⭐⭐⭐⭐⭐</p><h2 id="_1-4-speech-recognition-evolution" tabindex="-1"><a class="header-anchor" href="#_1-4-speech-recognition-evolution" aria-hidden="true">#</a> 1.4 Speech Recognition Evolution</h2><p><strong>[8]</strong> Hinton, Geoffrey, et al. &quot;<strong>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</strong>.&quot; IEEE Signal Processing Magazine 29.6 (2012): 82-97. <a href="http://cs224d.stanford.edu/papers/maas_paper.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Breakthrough in speech recognition)</strong>⭐⭐⭐⭐</p><p><strong>[9]</strong> Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. &quot;<strong>Speech recognition with deep recurrent neural networks</strong>.&quot; 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. <a href="http://arxiv.org/pdf/1303.5778.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(RNN)</strong>⭐⭐⭐</p><p><strong>[10]</strong> Graves, Alex, and Navdeep Jaitly. &quot;<strong>Towards End-To-End Speech Recognition with Recurrent Neural Networks</strong>.&quot; ICML. Vol. 14. 2014. <a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐</p><p><strong>[11]</strong> Sak, Haşim, et al. &quot;<strong>Fast and accurate recurrent neural network acoustic models for speech recognition</strong>.&quot; arXiv preprint arXiv:1507.06947 (2015). <a href="http://arxiv.org/pdf/1507.06947" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Google Speech Recognition System)</strong> ⭐⭐⭐</p><p><strong>[12]</strong> Amodei, Dario, et al. &quot;<strong>Deep speech 2: End-to-end speech recognition in english and mandarin</strong>.&quot; arXiv preprint arXiv:1512.02595 (2015). <a href="https://arxiv.org/pdf/1512.02595.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Baidu Speech Recognition System)</strong> ⭐⭐⭐⭐</p><p><strong>[13]</strong> W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig &quot;<strong>Achieving Human Parity in Conversational Speech Recognition</strong>.&quot; arXiv preprint arXiv:1610.05256 (2016). <a href="https://arxiv.org/pdf/1610.05256v1" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(State-of-the-art in speech recognition, Microsoft)</strong> ⭐⭐⭐⭐</p><blockquote><p>After reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction.</p></blockquote><p>#2 Deep Learning Method</p><h2 id="_2-1-model" tabindex="-1"><a class="header-anchor" href="#_2-1-model" aria-hidden="true">#</a> 2.1 Model</h2><p><strong>[14]</strong> Hinton, Geoffrey E., et al. &quot;<strong>Improving neural networks by preventing co-adaptation of feature detectors</strong>.&quot; arXiv preprint arXiv:1207.0580 (2012). <a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Dropout)</strong> ⭐⭐⭐</p><p><strong>[15]</strong> Srivastava, Nitish, et al. &quot;<strong>Dropout: a simple way to prevent neural networks from overfitting</strong>.&quot; Journal of Machine Learning Research 15.1 (2014): 1929-1958. <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[16]</strong> Ioffe, Sergey, and Christian Szegedy. &quot;<strong>Batch normalization: Accelerating deep network training by reducing internal covariate shift</strong>.&quot; arXiv preprint arXiv:1502.03167 (2015). <a href="http://arxiv.org/pdf/1502.03167" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(An outstanding Work in 2015)</strong> ⭐⭐⭐⭐</p><p><strong>[17]</strong> Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. &quot;<strong>Layer normalization</strong>.&quot; arXiv preprint arXiv:1607.06450 (2016). <a href="https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&amp;utm_medium=refer&amp;utm_campaign=promote" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Update of Batch Normalization)</strong> ⭐⭐⭐⭐</p><p><strong>[18]</strong> Courbariaux, Matthieu, et al. &quot;<strong>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1</strong>.&quot; <a href="https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(New Model,Fast)</strong> ⭐⭐⭐</p><p><strong>[19]</strong> Jaderberg, Max, et al. &quot;<strong>Decoupled neural interfaces using synthetic gradients</strong>.&quot; arXiv preprint arXiv:1608.05343 (2016). <a href="https://arxiv.org/pdf/1608.05343" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Innovation of Training Method,Amazing Work)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[20]</strong> Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. &quot;Net2net: Accelerating learning via knowledge transfer.&quot; arXiv preprint arXiv:1511.05641 (2015). <a href="https://arxiv.org/abs/1511.05641" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Modify previously trained network to reduce training epochs)</strong> ⭐⭐⭐</p><p><strong>[21]</strong> Wei, Tao, et al. &quot;Network Morphism.&quot; arXiv preprint arXiv:1603.01670 (2016). <a href="https://arxiv.org/abs/1603.01670" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Modify previously trained network to reduce training epochs)</strong> ⭐⭐⭐</p><h2 id="_2-2-optimization" tabindex="-1"><a class="header-anchor" href="#_2-2-optimization" aria-hidden="true">#</a> 2.2 Optimization</h2><p><strong>[22]</strong> Sutskever, Ilya, et al. &quot;<strong>On the importance of initialization and momentum in deep learning</strong>.&quot; ICML (3) 28 (2013): 1139-1147. <a href="http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Momentum optimizer)</strong> ⭐⭐</p><p><strong>[23]</strong> Kingma, Diederik, and Jimmy Ba. &quot;<strong>Adam: A method for stochastic optimization</strong>.&quot; arXiv preprint arXiv:1412.6980 (2014). <a href="http://arxiv.org/pdf/1412.6980" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Maybe used most often currently)</strong> ⭐⭐⭐</p><p><strong>[24]</strong> Andrychowicz, Marcin, et al. &quot;<strong>Learning to learn by gradient descent by gradient descent</strong>.&quot; arXiv preprint arXiv:1606.04474 (2016). <a href="https://arxiv.org/pdf/1606.04474" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Neural Optimizer,Amazing Work)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[25]</strong> Han, Song, Huizi Mao, and William J. Dally. &quot;<strong>Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</strong>.&quot; CoRR, abs/1510.00149 2 (2015). <a href="https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[26]</strong> Iandola, Forrest N., et al. &quot;<strong>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size</strong>.&quot; arXiv preprint arXiv:1602.07360 (2016). <a href="http://arxiv.org/pdf/1602.07360" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Also a new direction to optimize NN,DeePhi Tech Startup)</strong> ⭐⭐⭐⭐</p><h2 id="_2-3-unsupervised-learning-deep-generative-model" tabindex="-1"><a class="header-anchor" href="#_2-3-unsupervised-learning-deep-generative-model" aria-hidden="true">#</a> 2.3 Unsupervised Learning / Deep Generative Model</h2><p><strong>[27]</strong> Le, Quoc V. &quot;<strong>Building high-level features using large scale unsupervised learning</strong>.&quot; 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. <a href="http://arxiv.org/pdf/1112.6209.pdf&amp;embed" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Milestone, Andrew Ng, Google Brain Project, Cat)</strong> ⭐⭐⭐⭐</p><p><strong>[28]</strong> Kingma, Diederik P., and Max Welling. &quot;<strong>Auto-encoding variational bayes</strong>.&quot; arXiv preprint arXiv:1312.6114 (2013). <a href="http://arxiv.org/pdf/1312.6114" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(VAE)</strong> ⭐⭐⭐⭐</p><p><strong>[29]</strong> Goodfellow, Ian, et al. &quot;<strong>Generative adversarial nets</strong>.&quot; Advances in Neural Information Processing Systems. 2014. <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(GAN,super cool idea)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[30]</strong> Radford, Alec, Luke Metz, and Soumith Chintala. &quot;<strong>Unsupervised representation learning with deep convolutional generative adversarial networks</strong>.&quot; arXiv preprint arXiv:1511.06434 (2015). <a href="http://arxiv.org/pdf/1511.06434" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(DCGAN)</strong> ⭐⭐⭐⭐</p><p><strong>[31]</strong> Gregor, Karol, et al. &quot;<strong>DRAW: A recurrent neural network for image generation</strong>.&quot; arXiv preprint arXiv:1502.04623 (2015). <a href="http://jmlr.org/proceedings/papers/v37/gregor15.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(VAE with attention, outstanding work)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[32]</strong> Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. &quot;<strong>Pixel recurrent neural networks</strong>.&quot; arXiv preprint arXiv:1601.06759 (2016). <a href="http://arxiv.org/pdf/1601.06759" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(PixelRNN)</strong> ⭐⭐⭐⭐</p><p><strong>[33]</strong> Oord, Aaron van den, et al. &quot;Conditional image generation with PixelCNN decoders.&quot; arXiv preprint arXiv:1606.05328 (2016). <a href="https://arxiv.org/pdf/1606.05328" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(PixelCNN)</strong> ⭐⭐⭐⭐</p><h2 id="_2-4-rnn-sequence-to-sequence-model" tabindex="-1"><a class="header-anchor" href="#_2-4-rnn-sequence-to-sequence-model" aria-hidden="true">#</a> 2.4 RNN / Sequence-to-Sequence Model</h2><p><strong>[34]</strong> Graves, Alex. &quot;<strong>Generating sequences with recurrent neural networks</strong>.&quot; arXiv preprint arXiv:1308.0850 (2013). <a href="http://arxiv.org/pdf/1308.0850" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(LSTM, very nice generating result, show the power of RNN)</strong> ⭐⭐⭐⭐</p><p><strong>[35]</strong> Cho, Kyunghyun, et al. &quot;<strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation</strong>.&quot; arXiv preprint arXiv:1406.1078 (2014). <a href="http://arxiv.org/pdf/1406.1078" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(First Seq-to-Seq Paper)</strong> ⭐⭐⭐⭐</p><p><strong>[36]</strong> Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. &quot;<strong>Sequence to sequence learning with neural networks</strong>.&quot; Advances in neural information processing systems. 2014. <a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Outstanding Work)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[37]</strong> Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. &quot;<strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong>.&quot; arXiv preprint arXiv:1409.0473 (2014). <a href="https://arxiv.org/pdf/1409.0473v7.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[38]</strong> Vinyals, Oriol, and Quoc Le. &quot;<strong>A neural conversational model</strong>.&quot; arXiv preprint arXiv:1506.05869 (2015). <a href="http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Seq-to-Seq on Chatbot)</strong> ⭐⭐⭐</p><h2 id="_2-5-neural-turing-machine" tabindex="-1"><a class="header-anchor" href="#_2-5-neural-turing-machine" aria-hidden="true">#</a> 2.5 Neural Turing Machine</h2><p><strong>[39]</strong> Graves, Alex, Greg Wayne, and Ivo Danihelka. &quot;<strong>Neural turing machines</strong>.&quot; arXiv preprint arXiv:1410.5401 (2014). <a href="http://arxiv.org/pdf/1410.5401.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Basic Prototype of Future Computer)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[40]</strong> Zaremba, Wojciech, and Ilya Sutskever. &quot;<strong>Reinforcement learning neural Turing machines</strong>.&quot; arXiv preprint arXiv:1505.00521 362 (2015). <a href="https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[41]</strong> Weston, Jason, Sumit Chopra, and Antoine Bordes. &quot;<strong>Memory networks</strong>.&quot; arXiv preprint arXiv:1410.3916 (2014). <a href="http://arxiv.org/pdf/1410.3916" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[42]</strong> Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. &quot;<strong>End-to-end memory networks</strong>.&quot; Advances in neural information processing systems. 2015. <a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[43]</strong> Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. &quot;<strong>Pointer networks</strong>.&quot; Advances in Neural Information Processing Systems. 2015. <a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[44]</strong> Graves, Alex, et al. &quot;<strong>Hybrid computing using a neural network with dynamic external memory</strong>.&quot; Nature (2016). <a href="https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Milestone,combine above papers&#39; ideas)</strong> ⭐⭐⭐⭐⭐</p><h2 id="_2-6-deep-reinforcement-learning" tabindex="-1"><a class="header-anchor" href="#_2-6-deep-reinforcement-learning" aria-hidden="true">#</a> 2.6 Deep Reinforcement Learning</h2><p><strong>[45]</strong> Mnih, Volodymyr, et al. &quot;<strong>Playing atari with deep reinforcement learning</strong>.&quot; arXiv preprint arXiv:1312.5602 (2013). <a href="http://arxiv.org/pdf/1312.5602.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>) <strong>(First Paper named deep reinforcement learning)</strong> ⭐⭐⭐⭐</p><p><strong>[46]</strong> Mnih, Volodymyr, et al. &quot;<strong>Human-level control through deep reinforcement learning</strong>.&quot; Nature 518.7540 (2015): 529-533. <a href="https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Milestone)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[47]</strong> Wang, Ziyu, Nando de Freitas, and Marc Lanctot. &quot;<strong>Dueling network architectures for deep reinforcement learning</strong>.&quot; arXiv preprint arXiv:1511.06581 (2015). <a href="http://arxiv.org/pdf/1511.06581" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(ICLR best paper,great idea)</strong> ⭐⭐⭐⭐</p><p><strong>[48]</strong> Mnih, Volodymyr, et al. &quot;<strong>Asynchronous methods for deep reinforcement learning</strong>.&quot; arXiv preprint arXiv:1602.01783 (2016). <a href="http://arxiv.org/pdf/1602.01783" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(State-of-the-art method)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[49]</strong> Lillicrap, Timothy P., et al. &quot;<strong>Continuous control with deep reinforcement learning</strong>.&quot; arXiv preprint arXiv:1509.02971 (2015). <a href="http://arxiv.org/pdf/1509.02971" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(DDPG)</strong> ⭐⭐⭐⭐</p><p><strong>[50]</strong> Gu, Shixiang, et al. &quot;<strong>Continuous Deep Q-Learning with Model-based Acceleration</strong>.&quot; arXiv preprint arXiv:1603.00748 (2016). <a href="http://arxiv.org/pdf/1603.00748" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(NAF)</strong> ⭐⭐⭐⭐</p><p><strong>[51]</strong> Schulman, John, et al. &quot;<strong>Trust region policy optimization</strong>.&quot; CoRR, abs/1502.05477 (2015). <a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(TRPO)</strong> ⭐⭐⭐⭐</p><p><strong>[52]</strong> Silver, David, et al. &quot;<strong>Mastering the game of Go with deep neural networks and tree search</strong>.&quot; Nature 529.7587 (2016): 484-489. <a href="http://willamette.edu/~levenick/cs448/goNature.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(AlphaGo)</strong> ⭐⭐⭐⭐⭐</p><h2 id="_2-7-deep-transfer-learning-lifelong-learning-especially-for-rl" tabindex="-1"><a class="header-anchor" href="#_2-7-deep-transfer-learning-lifelong-learning-especially-for-rl" aria-hidden="true">#</a> 2.7 Deep Transfer Learning / Lifelong Learning / especially for RL</h2><p><strong>[53]</strong> Bengio, Yoshua. &quot;<strong>Deep Learning of Representations for Unsupervised and Transfer Learning</strong>.&quot; ICML Unsupervised and Transfer Learning 27 (2012): 17-36. <a href="http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(A Tutorial)</strong> ⭐⭐⭐</p><p><strong>[54]</strong> Silver, Daniel L., Qiang Yang, and Lianghao Li. &quot;<strong>Lifelong Machine Learning Systems: Beyond Learning Algorithms</strong>.&quot; AAAI Spring Symposium: Lifelong Machine Learning. 2013. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(A brief discussion about lifelong learning)</strong> ⭐⭐⭐</p><p><strong>[55]</strong> Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. &quot;<strong>Distilling the knowledge in a neural network</strong>.&quot; arXiv preprint arXiv:1503.02531 (2015). <a href="http://arxiv.org/pdf/1503.02531" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Godfather&#39;s Work)</strong> ⭐⭐⭐⭐</p><p><strong>[56]</strong> Rusu, Andrei A., et al. &quot;<strong>Policy distillation</strong>.&quot; arXiv preprint arXiv:1511.06295 (2015). <a href="http://arxiv.org/pdf/1511.06295" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(RL domain)</strong> ⭐⭐⭐</p><p><strong>[57]</strong> Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. &quot;<strong>Actor-mimic: Deep multitask and transfer reinforcement learning</strong>.&quot; arXiv preprint arXiv:1511.06342 (2015). <a href="http://arxiv.org/pdf/1511.06342" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(RL domain)</strong> ⭐⭐⭐</p><p><strong>[58]</strong> Rusu, Andrei A., et al. &quot;<strong>Progressive neural networks</strong>.&quot; arXiv preprint arXiv:1606.04671 (2016). <a href="https://arxiv.org/pdf/1606.04671" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Outstanding Work, A novel idea)</strong> ⭐⭐⭐⭐⭐</p><h2 id="_2-8-one-shot-deep-learning" tabindex="-1"><a class="header-anchor" href="#_2-8-one-shot-deep-learning" aria-hidden="true">#</a> 2.8 One Shot Deep Learning</h2><p><strong>[59]</strong> Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. &quot;<strong>Human-level concept learning through probabilistic program induction</strong>.&quot; Science 350.6266 (2015): 1332-1338. <a href="http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(No Deep Learning,but worth reading)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[60]</strong> Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. &quot;<strong>Siamese Neural Networks for One-shot Image Recognition</strong>.&quot;(2015) <a href="http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[61]</strong> Santoro, Adam, et al. &quot;<strong>One-shot Learning with Memory-Augmented Neural Networks</strong>.&quot; arXiv preprint arXiv:1605.06065 (2016). <a href="http://arxiv.org/pdf/1605.06065" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(A basic step to one shot learning)</strong> ⭐⭐⭐⭐</p><p><strong>[62]</strong> Vinyals, Oriol, et al. &quot;<strong>Matching Networks for One Shot Learning</strong>.&quot; arXiv preprint arXiv:1606.04080 (2016). <a href="https://arxiv.org/pdf/1606.04080" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[63]</strong> Hariharan, Bharath, and Ross Girshick. &quot;<strong>Low-shot visual object recognition</strong>.&quot; arXiv preprint arXiv:1606.02819 (2016). <a href="http://arxiv.org/pdf/1606.02819" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(A step to large data)</strong> ⭐⭐⭐⭐</p><h1 id="_3-applications" tabindex="-1"><a class="header-anchor" href="#_3-applications" aria-hidden="true">#</a> 3 Applications</h1><h2 id="_3-1-nlp-natural-language-processing" tabindex="-1"><a class="header-anchor" href="#_3-1-nlp-natural-language-processing" aria-hidden="true">#</a> 3.1 NLP(Natural Language Processing)</h2><p><strong>[1]</strong> Antoine Bordes, et al. &quot;<strong>Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing</strong>.&quot; AISTATS(2012) <a href="https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&amp;cache=cache&amp;media=en:bordes12aistats.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[2]</strong> Mikolov, et al. &quot;<strong>Distributed representations of words and phrases and their compositionality</strong>.&quot; ANIPS(2013): 3111-3119 <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(word2vec)</strong> ⭐⭐⭐</p><p><strong>[3]</strong> Sutskever, et al. &quot;<strong>“Sequence to sequence learning with neural networks</strong>.&quot; ANIPS(2014) <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[4]</strong> Ankit Kumar, et al. &quot;<strong>“Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</strong>.&quot; arXiv preprint arXiv:1506.07285(2015) <a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[5]</strong> Yoon Kim, et al. &quot;<strong>Character-Aware Neural Language Models</strong>.&quot; NIPS(2015) arXiv preprint arXiv:1508.06615(2015) <a href="https://arxiv.org/abs/1508.06615" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[6]</strong> Jason Weston, et al. &quot;<strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</strong>.&quot; arXiv preprint arXiv:1502.05698(2015) <a href="https://arxiv.org/abs/1502.05698" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(bAbI tasks)</strong> ⭐⭐⭐</p><p><strong>[7]</strong> Karl Moritz Hermann, et al. &quot;<strong>Teaching Machines to Read and Comprehend</strong>.&quot; arXiv preprint arXiv:1506.03340(2015) <a href="https://arxiv.org/abs/1506.03340" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(CNN/DailyMail cloze style questions)</strong> ⭐⭐</p><p><strong>[8]</strong> Alexis Conneau, et al. &quot;<strong>Very Deep Convolutional Networks for Natural Language Processing</strong>.&quot; arXiv preprint arXiv:1606.01781(2016) <a href="https://arxiv.org/abs/1606.01781" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(state-of-the-art in text classification)</strong> ⭐⭐⭐</p><p><strong>[9]</strong> Armand Joulin, et al. &quot;<strong>Bag of Tricks for Efficient Text Classification</strong>.&quot; arXiv preprint arXiv:1607.01759(2016) <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(slightly worse than state-of-the-art, but a lot faster)</strong> ⭐⭐⭐</p><h2 id="_3-2-object-detection" tabindex="-1"><a class="header-anchor" href="#_3-2-object-detection" aria-hidden="true">#</a> 3.2 Object Detection</h2><p><strong>[1]</strong> Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. &quot;<strong>Deep neural networks for object detection</strong>.&quot; Advances in Neural Information Processing Systems. 2013. <a href="http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[2]</strong> Girshick, Ross, et al. &quot;<strong>Rich feature hierarchies for accurate object detection and semantic segmentation</strong>.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(RCNN)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[3]</strong> He, Kaiming, et al. &quot;<strong>Spatial pyramid pooling in deep convolutional networks for visual recognition</strong>.&quot; European Conference on Computer Vision. Springer International Publishing, 2014. <a href="http://arxiv.org/pdf/1406.4729" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(SPPNet)</strong> ⭐⭐⭐⭐</p><p><strong>[4]</strong> Girshick, Ross. &quot;<strong>Fast r-cnn</strong>.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2015. <a href="https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[5]</strong> Ren, Shaoqing, et al. &quot;<strong>Faster R-CNN: Towards real-time object detection with region proposal networks</strong>.&quot; Advances in neural information processing systems. 2015. <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[6]</strong> Redmon, Joseph, et al. &quot;<strong>You only look once: Unified, real-time object detection</strong>.&quot; arXiv preprint arXiv:1506.02640 (2015). <a href="http://homes.cs.washington.edu/~ali/papers/YOLO.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(YOLO,Oustanding Work, really practical)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[7]</strong> Liu, Wei, et al. &quot;<strong>SSD: Single Shot MultiBox Detector</strong>.&quot; arXiv preprint arXiv:1512.02325 (2015). <a href="http://arxiv.org/pdf/1512.02325" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[8]</strong> Dai, Jifeng, et al. &quot;<strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks</strong>.&quot; arXiv preprint arXiv:1605.06409 (2016). <a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[9]</strong> He, Gkioxari, et al. &quot;<strong>Mask R-CNN</strong>&quot; arXiv preprint arXiv:1703.06870 (2017). <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><h2 id="_3-3-visual-tracking" tabindex="-1"><a class="header-anchor" href="#_3-3-visual-tracking" aria-hidden="true">#</a> 3.3 Visual Tracking</h2><p><strong>[1]</strong> Wang, Naiyan, and Dit-Yan Yeung. &quot;<strong>Learning a deep compact image representation for visual tracking</strong>.&quot; Advances in neural information processing systems. 2013. <a href="http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(First Paper to do visual tracking using Deep Learning,DLT Tracker)</strong> ⭐⭐⭐</p><p><strong>[2]</strong> Wang, Naiyan, et al. &quot;<strong>Transferring rich feature hierarchies for robust visual tracking</strong>.&quot; arXiv preprint arXiv:1501.04587 (2015). <a href="http://arxiv.org/pdf/1501.04587" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(SO-DLT)</strong> ⭐⭐⭐⭐</p><p><strong>[3]</strong> Wang, Lijun, et al. &quot;<strong>Visual tracking with fully convolutional networks</strong>.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2015. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(FCNT)</strong> ⭐⭐⭐⭐</p><p><strong>[4]</strong> Held, David, Sebastian Thrun, and Silvio Savarese. &quot;<strong>Learning to Track at 100 FPS with Deep Regression Networks</strong>.&quot; arXiv preprint arXiv:1604.01802 (2016). <a href="http://arxiv.org/pdf/1604.01802" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods)</strong> ⭐⭐⭐⭐</p><p><strong>[5]</strong> Bertinetto, Luca, et al. &quot;<strong>Fully-Convolutional Siamese Networks for Object Tracking</strong>.&quot; arXiv preprint arXiv:1606.09549 (2016). <a href="https://arxiv.org/pdf/1606.09549" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(SiameseFC,New state-of-the-art for real-time object tracking)</strong> ⭐⭐⭐⭐</p><p><strong>[6]</strong> Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. &quot;<strong>Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking</strong>.&quot; ECCV (2016) <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(C-COT)</strong> ⭐⭐⭐⭐</p><p><strong>[7]</strong> Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. &quot;<strong>Modeling and Propagating CNNs in a Tree Structure for Visual Tracking</strong>.&quot; arXiv preprint arXiv:1608.07242 (2016). <a href="https://arxiv.org/pdf/1608.07242" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(VOT2016 Winner,TCNN)</strong> ⭐⭐⭐⭐</p><h2 id="_3-4-image-caption" tabindex="-1"><a class="header-anchor" href="#_3-4-image-caption" aria-hidden="true">#</a> 3.4 Image Caption</h2><p><strong>[1]</strong> Farhadi,Ali,etal. &quot;<strong>Every picture tells a story: Generating sentences from images</strong>&quot;. In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. <a href="https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[2]</strong> Kulkarni, Girish, et al. &quot;<strong>Baby talk: Understanding and generating image descriptions</strong>&quot;. In Proceedings of the 24th CVPR, 2011. <a href="http://tamaraberg.com/papers/generation_cvpr11.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐⭐</p><p><strong>[3]</strong> Vinyals, Oriol, et al. &quot;<strong>Show and tell: A neural image caption generator</strong>&quot;. In arXiv preprint arXiv:1411.4555, 2014. <a href="https://arxiv.org/pdf/1411.4555.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐</p><p><strong>[4]</strong> Donahue, Jeff, et al. &quot;<strong>Long-term recurrent convolutional networks for visual recognition and description</strong>&quot;. In arXiv preprint arXiv:1411.4389 ,2014. <a href="https://arxiv.org/pdf/1411.4389.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>[5]</strong> Karpathy, Andrej, and Li Fei-Fei. &quot;<strong>Deep visual-semantic alignments for generating image descriptions</strong>&quot;. In arXiv preprint arXiv:1412.2306, 2014. <a href="https://cs.stanford.edu/people/karpathy/cvpr2015.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐⭐⭐</p><p><strong>[6]</strong> Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. &quot;<strong>Deep fragment embeddings for bidirectional image sentence mapping</strong>&quot;. In Advances in neural information processing systems, 2014. <a href="https://arxiv.org/pdf/1406.5679v1.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐⭐</p><p><strong>[7]</strong> Fang, Hao, et al. &quot;<strong>From captions to visual concepts and back</strong>&quot;. In arXiv preprint arXiv:1411.4952, 2014. <a href="https://arxiv.org/pdf/1411.4952v3.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐⭐⭐</p><p><strong>[8]</strong> Chen, Xinlei, and C. Lawrence Zitnick. &quot;<strong>Learning a recurrent visual representation for image caption generation</strong>&quot;. In arXiv preprint arXiv:1411.5654, 2014. <a href="https://arxiv.org/pdf/1411.5654v1.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐⭐</p><p><strong>[9]</strong> Mao, Junhua, et al. &quot;<strong>Deep captioning with multimodal recurrent neural networks (m-rnn)</strong>&quot;. In arXiv preprint arXiv:1412.6632, 2014. <a href="https://arxiv.org/pdf/1412.6632v5.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐</p><p><strong>[10]</strong> Xu, Kelvin, et al. &quot;<strong>Show, attend and tell: Neural image caption generation with visual attention</strong>&quot;. In arXiv preprint arXiv:1502.03044, 2015. <a href="https://arxiv.org/pdf/1502.03044v3.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐⭐⭐</p><h2 id="_3-5-machine-translation" tabindex="-1"><a class="header-anchor" href="#_3-5-machine-translation" aria-hidden="true">#</a> 3.5 Machine Translation</h2><blockquote><p>Some milestone papers are listed in RNN / Seq-to-Seq topic.</p></blockquote><p><strong>[1]</strong> Luong, Minh-Thang, et al. &quot;<strong>Addressing the rare word problem in neural machine translation</strong>.&quot; arXiv preprint arXiv:1410.8206 (2014). <a href="http://arxiv.org/pdf/1410.8206" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[2]</strong> Sennrich, et al. &quot;<strong>Neural Machine Translation of Rare Words with Subword Units</strong>&quot;. In arXiv preprint arXiv:1508.07909, 2015. <a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐</p><p><strong>[3]</strong> Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. &quot;<strong>Effective approaches to attention-based neural machine translation</strong>.&quot; arXiv preprint arXiv:1508.04025 (2015). <a href="http://arxiv.org/pdf/1508.04025" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[4]</strong> Chung, et al. &quot;<strong>A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</strong>&quot;. In arXiv preprint arXiv:1603.06147, 2016. <a href="https://arxiv.org/pdf/1603.06147.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐</p><p><strong>[5]</strong> Lee, et al. &quot;<strong>Fully Character-Level Neural Machine Translation without Explicit Segmentation</strong>&quot;. In arXiv preprint arXiv:1610.03017, 2016. <a href="https://arxiv.org/pdf/1610.03017.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>⭐⭐⭐⭐⭐</p><p><strong>[6]</strong> Wu, Schuster, Chen, Le, et al. &quot;<strong>Google&#39;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</strong>&quot;. In arXiv preprint arXiv:1609.08144v2, 2016. <a href="https://arxiv.org/pdf/1609.08144v2.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Milestone)</strong> ⭐⭐⭐⭐</p><h2 id="_3-6-robotics" tabindex="-1"><a class="header-anchor" href="#_3-6-robotics" aria-hidden="true">#</a> 3.6 Robotics</h2><p><strong>[1]</strong> Koutník, Jan, et al. &quot;<strong>Evolving large-scale neural networks for vision-based reinforcement learning</strong>.&quot; Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. <a href="http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[2]</strong> Levine, Sergey, et al. &quot;<strong>End-to-end training of deep visuomotor policies</strong>.&quot; Journal of Machine Learning Research 17.39 (2016): 1-40. <a href="http://www.jmlr.org/papers/volume17/15-522/15-522.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐⭐</p><p><strong>[3]</strong> Pinto, Lerrel, and Abhinav Gupta. &quot;<strong>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</strong>.&quot; arXiv preprint arXiv:1509.06825 (2015). <a href="http://arxiv.org/pdf/1509.06825" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[4]</strong> Levine, Sergey, et al. &quot;<strong>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</strong>.&quot; arXiv preprint arXiv:1603.02199 (2016). <a href="http://arxiv.org/pdf/1603.02199" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[5]</strong> Zhu, Yuke, et al. &quot;<strong>Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning</strong>.&quot; arXiv preprint arXiv:1609.05143 (2016). <a href="https://arxiv.org/pdf/1609.05143" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[6]</strong> Yahya, Ali, et al. &quot;<strong>Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search</strong>.&quot; arXiv preprint arXiv:1610.00673 (2016). <a href="https://arxiv.org/pdf/1610.00673" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[7]</strong> Gu, Shixiang, et al. &quot;<strong>Deep Reinforcement Learning for Robotic Manipulation</strong>.&quot; arXiv preprint arXiv:1610.00633 (2016). <a href="https://arxiv.org/pdf/1610.00633" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[8]</strong> A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.&quot;<strong>Sim-to-Real Robot Learning from Pixels with Progressive Nets</strong>.&quot; arXiv preprint arXiv:1610.04286 (2016). <a href="https://arxiv.org/pdf/1610.04286.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[9]</strong> Mirowski, Piotr, et al. &quot;<strong>Learning to navigate in complex environments</strong>.&quot; arXiv preprint arXiv:1611.03673 (2016). <a href="https://arxiv.org/pdf/1611.03673" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><h2 id="_3-7-art" tabindex="-1"><a class="header-anchor" href="#_3-7-art" aria-hidden="true">#</a> 3.7 Art</h2><p><strong>[1]</strong> Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). &quot;<strong>Inceptionism: Going Deeper into Neural Networks</strong>&quot;. Google Research. <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" target="_blank" rel="noopener noreferrer">[html]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Deep Dream)</strong> ⭐⭐⭐⭐</p><p><strong>[2]</strong> Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. &quot;<strong>A neural algorithm of artistic style</strong>.&quot; arXiv preprint arXiv:1508.06576 (2015). <a href="http://arxiv.org/pdf/1508.06576" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Outstanding Work, most successful method currently)</strong> ⭐⭐⭐⭐⭐</p><p><strong>[3]</strong> Zhu, Jun-Yan, et al. &quot;<strong>Generative Visual Manipulation on the Natural Image Manifold</strong>.&quot; European Conference on Computer Vision. Springer International Publishing, 2016. <a href="https://arxiv.org/pdf/1609.03552" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(iGAN)</strong> ⭐⭐⭐⭐</p><p><strong>[4]</strong> Champandard, Alex J. &quot;<strong>Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks</strong>.&quot; arXiv preprint arXiv:1603.01768 (2016). <a href="http://arxiv.org/pdf/1603.01768" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(Neural Doodle)</strong> ⭐⭐⭐⭐</p><p><strong>[5]</strong> Zhang, Richard, Phillip Isola, and Alexei A. Efros. &quot;<strong>Colorful Image Colorization</strong>.&quot; arXiv preprint arXiv:1603.08511 (2016). <a href="http://arxiv.org/pdf/1603.08511" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[6]</strong> Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. &quot;<strong>Perceptual losses for real-time style transfer and super-resolution</strong>.&quot; arXiv preprint arXiv:1603.08155 (2016). <a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[7]</strong> Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. &quot;<strong>A learned representation for artistic style</strong>.&quot; arXiv preprint arXiv:1610.07629 (2016). <a href="https://arxiv.org/pdf/1610.07629v1.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[8]</strong> Gatys, Leon and Ecker, et al.&quot;<strong>Controlling Perceptual Factors in Neural Style Transfer</strong>.&quot; arXiv preprint arXiv:1611.07865 (2016). <a href="https://arxiv.org/pdf/1611.07865.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(control style transfer over spatial location,colour information and across spatial scale)</strong>⭐⭐⭐⭐</p><p><strong>[9]</strong> Ulyanov, Dmitry and Lebedev, Vadim, et al. &quot;<strong>Texture Networks: Feed-forward Synthesis of Textures and Stylized Images</strong>.&quot; arXiv preprint arXiv:1603.03417(2016). <a href="http://arxiv.org/abs/1603.03417" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <strong>(texture generation and style transfer)</strong> ⭐⭐⭐⭐</p><h2 id="_3-8-object-segmentation" tabindex="-1"><a class="header-anchor" href="#_3-8-object-segmentation" aria-hidden="true">#</a> 3.8 Object Segmentation</h2><p><strong>[1]</strong> J. Long, E. Shelhamer, and T. Darrell, “<strong>Fully convolutional networks for semantic segmentation</strong>.” in CVPR, 2015. <a href="https://arxiv.org/pdf/1411.4038v2.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐⭐</p><p><strong>[2]</strong> L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. &quot;<strong>Semantic image segmentation with deep convolutional nets and fully connected crfs</strong>.&quot; In ICLR, 2015. <a href="https://arxiv.org/pdf/1606.00915v1.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐⭐</p><p><strong>[3]</strong> Pinheiro, P.O., Collobert, R., Dollar, P. &quot;<strong>Learning to segment object candidates.</strong>&quot; In: NIPS. 2015. <a href="https://arxiv.org/pdf/1506.06204v2.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐⭐</p><p><strong>[4]</strong> Dai, J., He, K., Sun, J. &quot;<strong>Instance-aware semantic segmentation via multi-task network cascades</strong>.&quot; in CVPR. 2016 <a href="https://arxiv.org/pdf/1512.04412v1.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><p><strong>[5]</strong> Dai, J., He, K., Sun, J. &quot;<strong>Instance-sensitive Fully Convolutional Networks</strong>.&quot; arXiv preprint arXiv:1603.08678 (2016). <a href="https://arxiv.org/pdf/1603.08678v1.pdf" target="_blank" rel="noopener noreferrer">[pdf]<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> ⭐⭐⭐</p><!--]--></div><footer class="page-meta"><div class="meta-item edit-link"><a class="external-link meta-item-label" href="https://github.com/ZhaoYLong/vuepress-starter/edit/main/articles/2019-09-26-Deep Learning Papers Reading Roadmap.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><!--]--> Edit this page <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><div class="meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><div class="meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: 2268678583@qq.com">ZhaoYLong</span><!----><!--]--><!--]--></span></div></footer><nav class="page-nav"><p class="inner"><span class="prev"><a href="/articles/2019-09-25-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E8%BF%B0.html" class="" aria-label="卷积神经网络简述"><!--[--><!--]--> 卷积神经网络简述 <!--[--><!--]--></a></span><span class="next"><a href="/articles/2019-09-26-Learn%20Math%20Fast.html" class="" aria-label="Learn Math Fast"><!--[--><!--]--> Learn Math Fast <!--[--><!--]--></a></span></p></nav><!--[--><!--]--></main><!--]--></div><!----><!--]--></div>
    <script type="module" src="/assets/app.3547d211.js" defer></script>
  </body>
</html>
